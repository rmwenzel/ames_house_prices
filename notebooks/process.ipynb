{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing and cleaning the Ames housing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The original dataset is available [here](http://www.amstat.org/publications/jse/v19n3/decock/AmesHousing.xls). A version of the dataset is available [on Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques). This is the dataset we'll be working with.\n",
    " \n",
    "In this notebook, we'll do preliminary processing and cleaning of the original dataset. In later notebooks we'll [explore the cleaned data and select/engineer features](eda.ipynb/#Exploratory-analysis-of-Ames-housing-dataset) and [model and predict sale prices](model.ipynb/#Modeling-and-predicting-SalePrice). Local copies of all datasets are in `house_prices/data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set_style('whitegrid')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# add parent directory for importing custom classes\n",
    "pardir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(pardir)\n",
    "\n",
    "# custom class for data description\n",
    "from codes.process import DataDescription, HPDataFramePlus, DataDescription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Load and inspect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A description of the dataset variables is available in `data/data_description.txt`, but it requires a little bit of preprocessing. The custom augmented `dict` class `DataDescription` contains code to do this (see `house_prices/codes/preprocess.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = DataDescription('../data/data_description.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First five variable names\n",
    "list(desc.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Description': ' Identifies the type of dwelling involved in the sale.',\n",
       " 'Values': {'20': '1-STORY 1946 & NEWER ALL STYLES',\n",
       "  '30': '1-STORY 1945 & OLDER',\n",
       "  '40': '1-STORY W/FINISHED ATTIC ALL AGES',\n",
       "  '45': '1-1/2 STORY - UNFINISHED ALL AGES',\n",
       "  '50': '1-1/2 STORY FINISHED ALL AGES',\n",
       "  '60': '2-STORY 1946 & NEWER',\n",
       "  '70': '2-STORY 1945 & OLDER',\n",
       "  '75': '2-1/2 STORY ALL AGES',\n",
       "  '80': 'SPLIT OR MULTI-LEVEL',\n",
       "  '85': 'SPLIT FOYER',\n",
       "  '90': 'DUPLEX - ALL STYLES AND AGES',\n",
       "  '120': '1-STORY PUD (Planned Unit Development) - 1946 & NEWER',\n",
       "  '150': '1-1/2 STORY PUD - ALL AGES',\n",
       "  '160': '2-STORY PUD - 1946 & NEWER',\n",
       "  '180': 'PUD - MULTILEVEL - INCL SPLIT LEV/FOYER',\n",
       "  '190': '2 FAMILY CONVERSION - ALL STYLES AND AGES'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First variable description and values\n",
    "desc['MSSubClass']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load into `DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll combine training and test data into a single `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv', index_col='Id')\n",
    "test = pd.read_csv('../data/test.csv', index_col='Id')\n",
    "full = pd.concat([train, test], keys=['train', 'test'], axis=0, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td rowspan=\"5\" valign=\"top\">train</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "      Id                                                                    \n",
       "train 1           60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "      2           20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "      3           60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "      4           70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "      5           60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "         LandContour Utilities LotConfig  ... PoolArea PoolQC Fence  \\\n",
       "      Id                                  ...                         \n",
       "train 1          Lvl    AllPub    Inside  ...        0    NaN   NaN   \n",
       "      2          Lvl    AllPub       FR2  ...        0    NaN   NaN   \n",
       "      3          Lvl    AllPub    Inside  ...        0    NaN   NaN   \n",
       "      4          Lvl    AllPub    Corner  ...        0    NaN   NaN   \n",
       "      5          Lvl    AllPub       FR2  ...        0    NaN   NaN   \n",
       "\n",
       "         MiscFeature MiscVal MoSold  YrSold  SaleType  SaleCondition  \\\n",
       "      Id                                                               \n",
       "train 1          NaN       0      2    2008        WD         Normal   \n",
       "      2          NaN       0      5    2007        WD         Normal   \n",
       "      3          NaN       0      9    2008        WD         Normal   \n",
       "      4          NaN       0      2    2006        WD        Abnorml   \n",
       "      5          NaN       0     12    2008        WD         Normal   \n",
       "\n",
       "          SalePrice  \n",
       "      Id             \n",
       "train 1    208500.0  \n",
       "      2    181500.0  \n",
       "      3    223500.0  \n",
       "      4    140000.0  \n",
       "      5    250000.0  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 2919 entries, (train, 1) to (test, 2919)\n",
      "Data columns (total 80 columns):\n",
      "MSSubClass       2919 non-null int64\n",
      "MSZoning         2915 non-null object\n",
      "LotFrontage      2433 non-null float64\n",
      "LotArea          2919 non-null int64\n",
      "Street           2919 non-null object\n",
      "Alley            198 non-null object\n",
      "LotShape         2919 non-null object\n",
      "LandContour      2919 non-null object\n",
      "Utilities        2917 non-null object\n",
      "LotConfig        2919 non-null object\n",
      "LandSlope        2919 non-null object\n",
      "Neighborhood     2919 non-null object\n",
      "Condition1       2919 non-null object\n",
      "Condition2       2919 non-null object\n",
      "BldgType         2919 non-null object\n",
      "HouseStyle       2919 non-null object\n",
      "OverallQual      2919 non-null int64\n",
      "OverallCond      2919 non-null int64\n",
      "YearBuilt        2919 non-null int64\n",
      "YearRemodAdd     2919 non-null int64\n",
      "RoofStyle        2919 non-null object\n",
      "RoofMatl         2919 non-null object\n",
      "Exterior1st      2918 non-null object\n",
      "Exterior2nd      2918 non-null object\n",
      "MasVnrType       2895 non-null object\n",
      "MasVnrArea       2896 non-null float64\n",
      "ExterQual        2919 non-null object\n",
      "ExterCond        2919 non-null object\n",
      "Foundation       2919 non-null object\n",
      "BsmtQual         2838 non-null object\n",
      "BsmtCond         2837 non-null object\n",
      "BsmtExposure     2837 non-null object\n",
      "BsmtFinType1     2840 non-null object\n",
      "BsmtFinSF1       2918 non-null float64\n",
      "BsmtFinType2     2839 non-null object\n",
      "BsmtFinSF2       2918 non-null float64\n",
      "BsmtUnfSF        2918 non-null float64\n",
      "TotalBsmtSF      2918 non-null float64\n",
      "Heating          2919 non-null object\n",
      "HeatingQC        2919 non-null object\n",
      "CentralAir       2919 non-null object\n",
      "Electrical       2918 non-null object\n",
      "1stFlrSF         2919 non-null int64\n",
      "2ndFlrSF         2919 non-null int64\n",
      "LowQualFinSF     2919 non-null int64\n",
      "GrLivArea        2919 non-null int64\n",
      "BsmtFullBath     2917 non-null float64\n",
      "BsmtHalfBath     2917 non-null float64\n",
      "FullBath         2919 non-null int64\n",
      "HalfBath         2919 non-null int64\n",
      "BedroomAbvGr     2919 non-null int64\n",
      "KitchenAbvGr     2919 non-null int64\n",
      "KitchenQual      2918 non-null object\n",
      "TotRmsAbvGrd     2919 non-null int64\n",
      "Functional       2917 non-null object\n",
      "Fireplaces       2919 non-null int64\n",
      "FireplaceQu      1499 non-null object\n",
      "GarageType       2762 non-null object\n",
      "GarageYrBlt      2760 non-null float64\n",
      "GarageFinish     2760 non-null object\n",
      "GarageCars       2918 non-null float64\n",
      "GarageArea       2918 non-null float64\n",
      "GarageQual       2760 non-null object\n",
      "GarageCond       2760 non-null object\n",
      "PavedDrive       2919 non-null object\n",
      "WoodDeckSF       2919 non-null int64\n",
      "OpenPorchSF      2919 non-null int64\n",
      "EnclosedPorch    2919 non-null int64\n",
      "3SsnPorch        2919 non-null int64\n",
      "ScreenPorch      2919 non-null int64\n",
      "PoolArea         2919 non-null int64\n",
      "PoolQC           10 non-null object\n",
      "Fence            571 non-null object\n",
      "MiscFeature      105 non-null object\n",
      "MiscVal          2919 non-null int64\n",
      "MoSold           2919 non-null int64\n",
      "YrSold           2919 non-null int64\n",
      "SaleType         2918 non-null object\n",
      "SaleCondition    2919 non-null object\n",
      "SalePrice        1460 non-null float64\n",
      "dtypes: float64(12), int64(25), object(43)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "full.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some cleanup and preprocessing will be necessary. For example, there are quite a few missing values, and more than half the variables have been have been cast to `pandas` catch-all `object` dtype.\n",
    "\n",
    "About half of the data is training data and half is testing data - observations from the testing data have `NaN` values for `SalePrice`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 80)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of training data\n",
    "full.loc['train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 80)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of training data\n",
    "full.loc['test'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../data/ames.csv' does not exist: b'../data/ames.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-26747975d2e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/ames.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Order'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Id'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/house_prices/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/house_prices/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/house_prices/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/house_prices/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/house_prices/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../data/ames.csv' does not exist: b'../data/ames.csv'"
     ]
    }
   ],
   "source": [
    "ames = pd.read_csv('../data/ames.csv')\n",
    "ames = ames.drop(columns=['Unnamed: 0'])\n",
    "ames = ames.rename(columns={'Order': 'Id'})\n",
    "ames = ames.set_index('Id', drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: all the functions in [this section](#Drop-problematic-variables-and-observations) are rolled into `HPDataFramePlus` methods `encode_ords, drop_probs`\n",
    "\n",
    "Before we clean any data, we'll store the original dataset so we have an unadulaterated copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance of HPDataFramePlus for full dataset\n",
    "orig = HPDataFramePlus(data=full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify variables by kind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it doesn't affect the data, we'll group variables in the original dataset into categorical, ordinal and quantitative kinds. We'll use the custom class `HPDataFramePlus` which contains helpful methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set description attribute\n",
    "orig.desc = desc\n",
    "\n",
    "# view description of all variables except sale price\n",
    "cols = list(full.columns)\n",
    "cols.remove('SalePrice')\n",
    "orig.print_desc(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify the variables, there's really no alternative here than to carefully inspect the variable descriptions and determine which is which. To clarify our terms:\n",
    "\n",
    "- Categorical variables are discrete variables with no ordering (although they may have a numerical encoding)\n",
    "- Ordinal variables are discrete numeric variables, hence they have an ordering (and should be numerically encoded)\n",
    "- Quantiative variables are continuous numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split variables into categorical, ordinal, quantitative\n",
    "cat_cols = ['MSSubClass', 'MSZoning', 'Street', 'LandContour', 'LotConfig', 'Neighborhood', \n",
    "            'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', \n",
    "            'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating', 'CentralAir', \n",
    "            'Electrical', 'GarageType', 'MiscFeature', 'SaleType', 'SaleCondition', 'Alley']\n",
    "ord_cols = ['LotShape', 'Utilities', 'LandSlope', 'OverallQual', 'OverallCond', 'ExterQual', \n",
    "            'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
    "            'HeatingQC', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', \n",
    "            'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu',\n",
    "            'GarageFinish', 'GarageCars', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence',\n",
    "            'MoSold', 'YrSold']\n",
    "quant_cols = ['LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', \n",
    "              'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', \n",
    "              'GrLivArea', 'GarageYrBlt', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n",
    "              'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n",
    "              'SalePrice']\n",
    "\n",
    "# group columns by kind\n",
    "col_kinds = {'cat': cat_cols, 'ord': ord_cols, 'quant': quant_cols}\n",
    "\n",
    "# set col_kinds attribute\n",
    "orig.col_kinds = col_kinds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the cleaning begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe for cleaned data and set attributes\n",
    "clean = HPDataFramePlus(data=full)\n",
    "clean.col_kinds = orig.col_kinds\n",
    "clean.desc = orig.desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can clean, we need to make sure all variables are encoded appropriately. Let's compare the dtypes of our dataframe with the variable types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes for categorical variables\n",
    "clean.data[clean.col_kinds['cat']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes for ordinal variables\n",
    "clean.data[clean.col_kinds['ord']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes for quantitative variables\n",
    "clean.data[clean.col_kinds['quant']].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical and quantitative dtypes look good, but we'll need to deal with the ordinal variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect description of ordinal variables\n",
    "clean.print_desc(clean.col_kinds['ord'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon careful reflection, we'll encode the ordinal variable values by hand (taking care to distinguish between values of 0 and truly missing values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode ordinal variable values in dictionary by hand when needed\n",
    "ords = {}\n",
    "ords['GarageCond'] = {nan: 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
    "ords['BsmtCond'] = ords['GarageCond'].copy()\n",
    "ords['BsmtCond'].pop('Ex')\n",
    "ords['LandSlope'] = {'Gtl': 0, 'Mod': 1, 'Sev': 2}\n",
    "ords['PavedDrive'] = {'N': 0, 'P': 1, 'Y': 2}\n",
    "ords['GarageFinish'] = {nan: 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}\n",
    "ords['BsmtQual'] = {nan: 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}\n",
    "ords['GarageQual'] = ords['GarageCond'].copy()\n",
    "ords['LotShape'] = {'Reg': 0, 'IR1': 1, 'IR2': 2, 'IR3': 3}\n",
    "ords['Functional'] = {name: i for (i, name) in\n",
    "                      enumerate(reversed(\n",
    "                        orig.data['Functional'].unique()[:-1]))}\n",
    "ords['Functional'][nan] = nan\n",
    "ords['ExterCond'] = ords['GarageCond'].copy()\n",
    "ords['ExterQual'] = {'Fa': 0, 'TA': 1, 'Gd': 2, 'Ex': 3}\n",
    "ords['HeatingQC'] = ords['GarageCond'].copy()\n",
    "ords['KitchenQual'] = ords['BsmtQual'].copy()\n",
    "ords['BsmtFinType1'] = {nan: 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4,\n",
    "                        'ALQ': 5, 'GLQ': 6}\n",
    "ords['BsmtFinType2'] = ords['BsmtFinType1'].copy()\n",
    "ords['BsmtExposure'] = {nan: 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}\n",
    "ords['Fence'] = {nan: 0, 'MnPrv': 1, 'MnWw': 2, 'GdWo': 3, 'GdPrv': 4}\n",
    "ords['FireplaceQu'] = ords['GarageCond'].copy()\n",
    "ords['PoolQC'] = ords['BsmtQual'].copy()\n",
    "ords['Utilities'] = {nan: nan, 'ELO': 0, 'NoSeWa': 1, 'NoSewr': 2, 'AllPub': 3}\n",
    "\n",
    "# perform encoding\n",
    "clean.data = clean.encode_ords(mapper=ords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop problematic variables and observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all variables are properly encoded, we can drop those with too many missing values immediately. We'll be somewhat conservative and keep variables with 80% of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_mostly_missing_cols(hpdf):\n",
    "    \"\"\"Drop columns with too many missing values\"\"\"\n",
    "    copy = hpdf.data.copy()\n",
    "    # drop columns with more than 20% values missing\n",
    "    notna_col_mask = ~ (copy.isna().sum()/len(copy) > 0.20)\n",
    "    notna_col_mask.loc['SalePrice'] = True\n",
    "    copy = copy.loc[: , notna_col_mask]\n",
    "    # drop columns associated with those\n",
    "    copy.drop(columns=['MiscVal'])\n",
    "    return copy\n",
    "\n",
    "# create a new dataframe for cleaning\n",
    "clean.data = drop_mostly_missing_cols(clean)\n",
    "clean.update_col_kinds(clean.col_kinds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also drop some well-known outlying observations (at least, well-known on Kaggle ) in the training data. Dropping outliers is (for good reason) very controversial, and one should take great care in doing so. The justification for it depends on context, however. In our case, the end goal is to predict `SalePrice` accuractely. If dropping outliers improves the ability of our prediction model to generalize, than this may provide some retroactive justification.\n",
    "\n",
    "First we'll plot the outliers (identifying them by their relationship to `SalePrice`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outliers(hpdf):\n",
    "    \"\"\"Plot variables which contain well-known outliers.\"\"\"\n",
    "    plt.subplots(1, 2, figsize=(15, 10))\n",
    "    train = hpdf.data.loc['train', :]\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.scatterplot(x='OverallQual', y='SalePrice', data=train)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.scatterplot(x='GrLivArea', y='SalePrice', data=train)\n",
    "\n",
    "\n",
    "plot_outliers(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kagglers seem to frequently conclude that the outliers are the house with overall quality 4 but a sale price of more than \\\\$250,000, and the two houses with more than 4500 sq ft of general living area but sale prices less than \\\\$300,000.\n",
    "\n",
    "Whether this is well-justified, and how much it's an example of groupthink, is a matter for debate. But it seems to regularly improve the predictive capability of models, so we'll follow suit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_outliers(hpdf):\n",
    "    copy = hpdf.data.copy()\n",
    "    # drop outliers in OverallQual\n",
    "    idx = copy[(copy['OverallQual'] < 5) & (copy['SalePrice'] > 200000)].index[0][1]\n",
    "    copy = copy.drop(labels=[idx], axis=0, level='Id')\n",
    "    # drop outliers in GrLivArea\n",
    "    idx = copy[(copy['GrLivArea'] > 4000) & (copy['SalePrice'] < 300000)].index[0][1]\n",
    "    copy = copy.drop(labels=[idx], axis=0, level='Id')\n",
    "    \n",
    "    return copy\n",
    "\n",
    "clean.data = drop_outliers(clean)\n",
    "clean.update_col_kinds(clean.col_kinds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll see if there are any categorical variables with extremely unbalanced distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_unbal_dists(data, bal_threshold):\n",
    "    \"\"\"Print distributions of columns with more than bal_threshold proportion concentrated at a single value.\"\"\"\n",
    "    dists = []\n",
    "    for col in data.columns:\n",
    "        val_counts = data[col].value_counts()\n",
    "        dist = val_counts/sum(val_counts)\n",
    "        if dist.max() > bal_threshold:\n",
    "            dists += [dist]\n",
    "    for dist in dists:\n",
    "        print()\n",
    "        print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is missing a lot of values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts of missing values by variable, excluding SalePrice\n",
    "clean.na_counts().drop('SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect train and test distributions of missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Before we get into imputing them, to inform our choice of methods, let's see how their distributions might differ across training and test sets. We want to be careful imputing missing values when those missing values are distributed unevenly across train and test sets if our goal is prediction, since our imputation could introduce further difference between train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_and_test_missing_values(hpdf):\n",
    "    \"\"\"plot distribution of missing train values.\"\"\"\n",
    "    \n",
    "    copy = hpdf.data.drop(columns=['SalePrice'])\n",
    "    train = HPDataFramePlus(data=copy.loc['train', :])\n",
    "    test = HPDataFramePlus(data=copy.loc['test', :])\n",
    "    \n",
    "    fig, _ = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    train_missing_dist = train.na_counts()/train.na_counts().sum()\n",
    "    sns.barplot(x=train_missing_dist.index, y=train_missing_dist.values)\n",
    "    plt.xticks(rotation=75)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    test_missing_dist = test.na_counts()/test.na_counts().sum()\n",
    "    sns.barplot(x=test_missing_dist.index, y=test_missing_dist.values)\n",
    "    plt.xticks(rotation=75)\n",
    "\n",
    "plot_train_and_test_missing_values(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables missing values in train but not test set\n",
    "train_missing = HPDataFramePlus(data=clean.data.loc['train', :]).na_counts()\n",
    "test_missing = HPDataFramePlus(data=clean.data.loc['test', :]).na_counts()\n",
    "train_not_test = list(set(train_missing.index).difference(test_missing.index))\n",
    "train_not_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables missing values in test but not train set\n",
    "test_not_train = list(set(test_missing.index).difference(train_missing.index))\n",
    "test_not_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of variables missing values in train but not test set\n",
    "train_missing.loc[train_not_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of variables missing values in test but not train\n",
    "test_missing.loc[test_not_train].drop(index=['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are so few missing values for variables which are missing values in the train set not the test set (or vice versa), we won't worry about imputing them. \n",
    "\n",
    "Now let's look at the distributions of variables missing in both train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_both_train_and_test_missing_values(hpdf):\n",
    "    \"\"\"plot distribution of missing train values.\"\"\"\n",
    "    \n",
    "    copy = hpdf.data.drop(columns=['SalePrice'])\n",
    "    train_missing_dist = HPDataFramePlus(data=clean.data.loc['train', :]).na_counts()\n",
    "    train_missing_dist = train_missing_dist/sum(train_missing_dist)\n",
    "    \n",
    "    test_missing_dist = HPDataFramePlus(data=clean.data.loc['test', :]).na_counts()\n",
    "    test_missing_dist = test_missing_dist/sum(test_missing_dist)\n",
    "    \n",
    "    both_missing_index = set(train_missing_dist.index).intersection(test_missing_dist.index)\n",
    "    train_missing_dist = train_missing_dist.loc[both_missing_index]\n",
    "    test_missing_dist = test_missing_dist.loc[both_missing_index]\n",
    "    \n",
    "    fig, _ = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.barplot(x=train_missing_dist.index, y=train_missing_dist.values)\n",
    "    plt.xticks(rotation=60)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.barplot(x=test_missing_dist.index, y=test_missing_dist.values)\n",
    "    plt.xticks(rotation=60)\n",
    "\n",
    "plot_both_train_and_test_missing_values(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For variables missing values in both sets, the distributions are very similar, so we'll go ahead and impute these values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute small numbers of missing values by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation of missing values using point estimates (mean, mode, etc.) is very common but arguable somewhat crude. Since there are more sophisticated methods which aren't too difficult to use, we'd like to use them. They are however, a bit more computationally expensive. Since many of our variables are only missing a few values, imputing these values by hand using point estimates will cut down on computational cost while sacrificing little.\n",
    "\n",
    "An excellent, thorough treatment of imputation can be found in [Flexible Imputation of Missing Data](https://stefvanbuuren.name/fimd/) by Stef Van Buren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute variables with <= 4 missing values. Use mode for categoricals, median for quantitatives\n",
    "clean.data = clean.hand_impute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing value counts again\n",
    "clean.na_counts().drop(index=['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute missing categorical values with `XGBClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some methods for imputing missing categorical data are more common, e.g. multinomial classification, but any classifier will do. Given time and the inclination, one could explore different classifiers and try to estimate their imputation accuracy (e.g. by cross-validation on data with no missing values) but we won't do that here. Since `xgboost` classifier often performs very well with defaults, we'll use it to impute `MasVnrType` and `GarageType`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing categorical values with XGBClassifier\n",
    "clean.data = clean.impute_cats(response='SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing value counts again\n",
    "clean.na_counts().drop(index=['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute missing quantitative values with MICE and PMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Multiple Imputation with Chained Equations (MICE)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/) is principled method of imputing missing data. It can be combined with [Predictive Mean Matching (PMM)](http://stefvanbuuren.name/fimd/sec-pmm.html) to yield a powerful implementation method. One can find these methods implemented in Python in `statsmodels.imputation.mice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing quantitative values with MICE and PMM\n",
    "clean.data = clean.impute_quants(response='SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing value counts again\n",
    "clean.na_counts().drop(index=['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enforce dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of our `col_kinds` dictionary, we'll use pandas dtypes to track categorical, ordinal, and quantitative variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats, ords, quants = (clean.col_kinds['cat'], clean.col_kinds['ord'],\n",
    "                          clean.col_kinds['quant'])\n",
    "clean.data.loc[:, cats] = clean.data.loc[:, cats].astype('category')\n",
    "clean.data.loc[:, ords] = clean.data.loc[:, ords].astype('int64')\n",
    "clean.data.loc[:, 'MSSubClass'] = clean.data['MSSubClass'].astype(\n",
    "                                      'category')\n",
    "clean.data.loc[:, quants] = clean.data.loc[:, quants].astype('float64')\n",
    "clean.data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we'll save our datasets to disk. This will result in two files\n",
    "\n",
    "- `orig.csv` - Original train and test data combined in a single dataset, without any modification\n",
    "- `clean.csv` - Cleaned dataset, with problematic variables and observations dropped and missing values imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_data = DataPlus({'orig': orig, 'clean': clean})\n",
    "data_dir = '../data'\n",
    "hp_data.save_dfs(save_dir=data_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:house_prices]",
   "language": "python",
   "name": "conda-env-house_prices-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "287px",
    "left": "1065px",
    "top": "138px",
    "width": "159.359px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
